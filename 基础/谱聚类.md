# 谱聚类

## 方法：

​	点与点之间用边相连，距离远的点，边权值低，距离近的点，边权值高，再进行切分，使得子图间的权值低，子图内权值高，从而聚类



## 一、构建邻接矩阵：

### *ϵ*-邻近法：

设置阈值*ϵ*，并用欧氏距离S_ij度量两个点之间的距离。
$$
相似矩阵的S_{ij} = ||x_i-x_j||^2_2,再根据S_ij和\varepsilon的大小构建邻接矩阵W\\
w_ij = \left\{ \begin{array}{rcl}
		0,s_{ij}>\varepsilon \\ \epsilon ,s_{ij} \leq \epsilon\end{array}\right.
$$
其实类似逻辑回归思想，大于阈值就直接用值，小于就变成0

但不准确

### K邻近法：

利用KNN遍历所有样本点，取最近的k个点作为邻居，只有这k个点之间的W_ij > 0。

缺点：会导致构成的邻接矩阵非对称，所以需要转换成对称的

方法1:

​	只要一个点是属于另一个点的k个邻居之一，就保留欧式距离S_ij	
$$
w_ij = w_ji = \left\{\begin{array}{rcl} 0 \qquad x_i ∉ KNN(x_j)\; and \; x_j∉KNN(x_i) \\ exp(-\frac{||x_i - xj||^2_2}{2\sigma^2}) \qquad x_i ∈KNN(x_j) 
\; or \; x_j ∈ KNN(x_i)
			  \end{array}\right.
$$
方法2：

​	两个点必须互相是对方的k个邻居之一，才保留欧氏距离
$$
w_ij = w_ji = \left\{\begin{array}{rcl} 0 \qquad x_i ∉ KNN(x_j)\; or \; x_j∉KNN(x_i) \\ exp(-\frac{||x_i - xj||^2_2}{2\sigma^2}) \qquad x_i ∈KNN(x_j) 
\; and \; x_j ∈ KNN(x_i)
			  \end{array}\right.
$$

### 全连接法：

所有点的权值都大于0，可以选用不同核函数来定义边权重

分多项式核，高斯核，sigmoid核

使用高斯核函数RBF(径向基函数）时，相似矩阵与邻接矩阵相同：
$$
w_{ij} = s_{ij} = exp(-\frac{||x_i - x_j||^2_2}{2\sigma^2}) = e^{-\frac{||x_i - x_j||^2_2}{2\sigma^2}}\\
\sigma控制核函数作用范围，值越大影响范围越大
$$
高斯核函数：某种沿着径向对称的标量函数，定义为空间中的任意一点x到某一中心的x1之间的欧氏					  距离的单调函数，距离越大，值越小，就是上面公式

##  二、拉普拉斯矩阵：

L = D - W

## 三、无向图切图：

目标：将图切成相互没有连接的k个子图
$$
对于任意两个子图点的集合A,B\subset V，A\cap B = \emptyset，那么定义AB间的切图权重：\\
W(A,B) = \sum_{i\in A,j\in B} w_{ij}\\
对于k个子图集合A_i，...，A_k,切图cut为：\\
cut(A_1,A_2,...,A_k) = \frac{1}{2}\sum^{k}_{i=1}W(A_i,\overline A_i)
，\overline A_i 为A_i的补集\\即除了A_i子集以外的其他V的子集的并集
$$
所以目标转换成最小化cut，并找到best cut，但这样只能考虑到图之间的相似性最低，无法考虑到图内相似度最高。

## 四、切图聚类：

定义子集大小有两种方法：
$$
|A|:子集A包含的样本数
vol(A) = \sum_{i \in A}d_i(表示i节点的度)
$$
根据不同的定义，有不同的切图方法

### RatioCut切图：

​	对于每个切图，不光考虑最小化cut，还要同时考虑最大化每个子图点的个数
$$
RatioCut(A_1,...,A_k) = \frac{1}{2}\sum^{k}_{i=1}\frac{W(A_i,\overline A_i)}{|A_i|} = \sum^{k}_{i=1}\frac{cut(A_i,\overline A_i)}{|A_i|}\\
 = \sum_{i = 1}^{k}h_i^TLh_i = \sum_{i=1}^{k}(H^TLH) = tr(H^TLH)
$$
因此要最小化ratiocut
$$
指示向量定义为数量与子图数量相同，维度与子图中样本数相同\\
h = {h_1,...,h_k}，每个h_i = (h_{1,i},...,h_{n,i})\\
h_{i,j} = \left \{\begin{array}{rcl} 0 \qquad i \notin A_j \\
		  \frac{1}{\sqrt{|A_i|}} i \in A_j\end{array}\right.\\
指示向量是正交向量，因为模也是1，且两两正交
$$
所以就是最小化迹，就是找最小的特征值

在PCA中，我们的目标是找到协方差矩阵（对应此处的拉普拉斯矩阵L）的最大的特征值，而在我们的谱聚类中，我们的目标是找到目标的最小的特征值，得到对应的特征向量，此时对应二分切图效果最佳。

找最小的k个特征值，其对应的k个特征向量，组成n*k的矩阵，就是H，而且需要按行做标准化
$$
h^*_{ij} = \frac{h_{ij}}{(\sum^k_{t=1}h^2_{it})^{\frac{1}{2}}}
$$
只做k个特征值，会导致损失少量信息，所以要对H的每一行再进行传统聚类

### Ncut切图:

就是把RatioCut的分母换成vol，因为样本个数多不一定权重大，所以vol更能符合
$$
Ncut(A_1,...,A_k) = \frac{1}{2}\sum^k_{i=1}\frac{W(A_i,\overline A_i)}{vol(A_i)} = \frac {cut(A_i,\overline{A_i})}{vol(A_i)}\\
指示向量变为\left \{\begin{array}{rcl} 0 \qquad v_i \notin A_j\\
\frac{1}{\sqrt{vol(A_j)}}v_i \in A_j
\end{array}\right.\\
不是正交基，因为模不是1，所以H^TH \neq I，而H^TDH = I\\
所以为了方便计算，让H^TDH = U^TU = I，所以H = D^{-\frac{1}{2}}U,U是单位正交矩阵，\\
就转换成H^TLH = UD^{-\frac{1}{2}} L D^{-\frac{1}{2}}U,所以就是优化\\
tr(UD^{-\frac{1}{2}} L D^{-\frac{1}{2}}U)
$$

## 五、谱聚类流程：

最常用的相似矩阵的生成方式是基于高斯核距离的全连接方式，最常用的切图方式是Ncut。而到最后常用的聚类方法为K-Means。

输入：样本集D=(*x*1,*x*2,...,*x**n*)，相似矩阵的生成方式, 降维后的维度*k*1, 聚类方法，聚类后的维度*k*2

输出： 簇划分*C*(*c*1,*c*2,...*c**k*2)

　　　　1) 根据输入的相似矩阵的生成方式构建样本的相似矩阵S

　　　　2）根据相似矩阵S构建邻接矩阵W，构建度矩阵D

　　　　3）计算出拉普拉斯矩阵L

　　　　4）构建标准化后的拉普拉斯矩阵*D*^−1/2 *L * D^−1/2

​		   　5）计算*D*^−1/2  *L*  * D^−1/2最小的*k*1个特征值所各自对应的特征向量*f*

​			  6) 将各自对应的特征向量*f*组成的矩阵按行标准化，最终组成*n*×*k*1维的特征矩阵F

　　　　7）对F中的每一行作为一个*k*1维的样本，共n个样本，用输入的聚类方法进行聚类，聚类维数为*k*2

　　　　8）得到簇划分*C*(*c*1,*c*2,...*c**k*2)　　　　





谱聚类能够很好地处理各向异性地数据集和非凸数据集，相比传统k均值来说

### 1.非凸集的对比

​	-k均值

![image-20220613160155465](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220613160155465.png)

​	-谱聚类

![image-20220613160215244](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220613160215244.png)

​	-各向异性

![image-20220613160258773](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220613160258773.png)

### 2、高斯核参数调优

gamma = 10效果已经不错

![image-20220613160659639](C:\Users\chen_wink\AppData\Roaming\Typora\typora-user-images\image-20220613160659639.png)

## 六、一些问题：

### 簇类个数的选择：

常用法：启发式的特征值差值搜索

若前k个特征值很小，且第k+1个特征值与前一个特征值相差比较大，则簇类个数选择k。

```python
'''
auther:wink
2022-6-12
'''

#非凸数据集
from sklearn import datasets
from sklearn import cluster
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics


plt.figure(figsize=[6,6])
n_samples = 1500  # 样本数量
#生成训练样本，构建双环数据集
noisy_circles = datasets.make_circles(n_samples=n_samples,factor=.5,noise=.05)
plt.scatter(noisy_circles[0][:,0],noisy_circles[0][:,1],marker='.')
plt.title("non-convex datasets")
plt.show()

#k-means
y_pred = KMeans(n_clusters=2,random_state=3).fit_predict(noisy_circles[0])
plt.scatter(noisy_circles[0][:,0],noisy_circles[0][:,1],marker='.',c=y_pred)
plt.title("k-means clustering")
plt.show()

#谱聚类
y_pred = cluster.SpectralClustering(n_clusters=2,affinity="nearest_neighbors").fit_predict(noisy_circles[0])
plt.scatter(noisy_circles[0][:,0],noisy_circles[0][:,1],marker='.',c=y_pred)
plt.title("spectralClustering")
plt.show()

'''
对于rbf核的参数调优
'''
X,y = datasets.make_blobs(n_samples=n_samples,random_state=170)
transformation = [[0.6,-0.6],[-0.4,0.8]]
x_aniso = np.dot(X,transformation)
aniso = (x_aniso,y)
y_pred = cluster.SpectralClustering(n_clusters=3,affinity='rbf',gamma=10).fit_predict(x_aniso)
plt.scatter(x_aniso[:,0],x_aniso[:,1],marker='.',c=y_pred)
plt.show()

# for index, gamma in enumerate((0.01,0.1,1,10,15)):
#     y_pred = cluster.SpectralClustering(n_clusters=3, gamma=gamma).fit_predict(x_aniso)
#     print("Calinski-Harabasz Score with gamma=", gamma, "score:", metrics.calinski_harabasz_score(X, y_pred))

```





## 参考文献

 Luxburg U V . A Tutorial on Spectral Clustering[J]. Statistics and Computing, 2004, 17(4):395-416.

 https://www.cnblogs.com/pinard/p/6221564.html 

